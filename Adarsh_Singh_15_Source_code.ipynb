{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# Importing all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import string\n",
    "import itertools \n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, ngrams\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, LSTM\n",
    "from keras.layers import MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D,GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting training and test datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some EDA and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>z13lfzdo5vmdi1cm123te5uz2mqig1brz04</td>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>2013-11-27T21:39:24</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>z12avveb4xqiirsix04chxviiljryduwxg0</td>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28T16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID           AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU        Julius NM   \n",
       "1          z13jhp0bxqncu512g22wvzkasxmvvzjaz04  ElNino Melendez   \n",
       "2          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw           GsMega   \n",
       "3          z13lfzdo5vmdi1cm123te5uz2mqig1brz04   ferleck ferles   \n",
       "4          z12avveb4xqiirsix04chxviiljryduwxg0      BeBe Burkey   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "2  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "3  2013-11-27T21:39:24                          Subscribe to my channel ﻿   \n",
       "4  2013-11-28T16:30:13  and u should.d check my channel and tell me wh...   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID      0\n",
       "AUTHOR          0\n",
       "DATE          138\n",
       "CONTENT         0\n",
       "CLASS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values in train\n",
    "train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc</td>\n",
       "      <td>Jason Haddad</td>\n",
       "      <td>2013-11-26T02:55:11</td>\n",
       "      <td>Hey, check out my new website!! This site is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>z122wfnzgt30fhubn04cdn3xfx2mxzngsl40k</td>\n",
       "      <td>Bob Kanowski</td>\n",
       "      <td>2013-11-28T12:33:27</td>\n",
       "      <td>i turned it on mute as soon is i came on i jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>z13ttt1jcraqexk2o234ghbgzxymz1zzi04</td>\n",
       "      <td>Cony</td>\n",
       "      <td>2013-11-28T16:01:47</td>\n",
       "      <td>You should check my channel for Funny VIDEOS!!﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                   COMMENT_ID            AUTHOR  \\\n",
       "0   0  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "1   1  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "2   2  LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc      Jason Haddad   \n",
       "3   3        z122wfnzgt30fhubn04cdn3xfx2mxzngsl40k      Bob Kanowski   \n",
       "4   4          z13ttt1jcraqexk2o234ghbgzxymz1zzi04              Cony   \n",
       "\n",
       "                  DATE                                            CONTENT  \n",
       "0  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...  \n",
       "1  2013-11-08T17:34:21             just for test I have to say murdev.com  \n",
       "2  2013-11-26T02:55:11  Hey, check out my new website!! This site is a...  \n",
       "3  2013-11-28T12:33:27  i turned it on mute as soon is i came on i jus...  \n",
       "4  2013-11-28T16:01:47    You should check my channel for Funny VIDEOS!!﻿  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              0\n",
       "COMMENT_ID      0\n",
       "AUTHOR          0\n",
       "DATE          107\n",
       "CONTENT         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values in test\n",
    "test.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of Author in training :  1094\n",
      "Unique values of Author in test :  760\n"
     ]
    }
   ],
   "source": [
    "# Unique values of Author\n",
    "print('Unique values of Author in training : ', len(set(train['AUTHOR'].to_list())))\n",
    "print('Unique values of Author in test : ', len(set(test['AUTHOR'].to_list())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DATE column has some missing values in both train and test dataset but this will not have impact on prediction\n",
    "2. COMMENT_ID and DATE have no significance in prediction of CLASS so that should be removed\n",
    "3. For significance of Author, we must check the number of unique values in both datasets, number of rows and number of unique values in Author are almost similar which means that there is one unique author for each comment, so the Author column plays no significance in classifcation and should also be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    586\n",
      "0    571\n",
      "Name: CLASS, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dedd68eb88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Value count and histogram plot\n",
    "print(train['CLASS'].value_counts())\n",
    "train['CLASS'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of 0s and 1s are almost same in the training and test data which means that this is a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only considering relevant features (CONTENT, CLASS for train and ID, CONTENT for test) for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"CONTENT\", \"CLASS\"]]\n",
    "test = test[[\"ID\",\"CONTENT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As per my industrial knowledge, I think the words like \"like\", \"subscribe\", \"watch\", \"please\" and the weblinks containing rows is more likely an Ad as these type of words are used in a sentence when someone wants you to visit, like or subscribe their website, page or channel,.\n",
    "\n",
    "2. For this, we made a funtion which provides the number of occurence of a word, its value counts and a plot between class and frequency.\n",
    "\n",
    "3. First, we are going to run the funtion for the \"http\" as these are the most common characters indicating a weblink.\n",
    "\n",
    "4. After cleaning the text, we will be run the function to check my hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the occurence of the word, its value counts, comparison b/w class & frequency\n",
    "\n",
    "def analysis(i):\n",
    "    print( \"\\033[1m\" + 'Analysis for: ' + str(i) + \"\\033[0m\")\n",
    "    print()\n",
    "    print('Total Number of Occurence: ')\n",
    "    # number of occurence of the word\n",
    "    print(train.CONTENT.str.count(i).sum())\n",
    "    print()\n",
    "    print('Total Value Counts: ')\n",
    "    # total value counts of the word\n",
    "    print(train.CONTENT.str.count(i).value_counts())\n",
    "    print()\n",
    "    df_analys=pd.DataFrame()\n",
    "    df_analys['freq']=train.CONTENT.str.count(i).to_list()\n",
    "    df_analys['CLASS']=train['CLASS']\n",
    "    df_analys['mark']= [j for j in range(0,train.shape[0])]\n",
    "    print('Comparison Table: ')\n",
    "    print()\n",
    "    # comparison plot b/w class and frequency\n",
    "    print(df_analys.pivot_table(index='CLASS',columns='freq',aggfunc='count', values = 'mark'))\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAnalysis for: http\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "149\n",
      "\n",
      "Total Value Counts: \n",
      "0    1031\n",
      "1     111\n",
      "2      12\n",
      "7       1\n",
      "4       1\n",
      "3       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0      1     2    3    4    7\n",
      "CLASS                                   \n",
      "0      561.0    8.0   2.0  NaN  NaN  NaN\n",
      "1      470.0  103.0  10.0  1.0  1.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling the above funtion for 'http'\n",
    "analysis('http')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the above output, \"http\" has significance occurence and from the plot, it can be state that whemever \"http\" occurs, it is more likely to be an ad.\n",
    "Also, the more its occurence, the more chances of it being an ad.\n",
    "2. While cleaning the text, we will not remove the words containing \"http\", but it has to be replaced with something which can distinguish it and add value while training.\n",
    "3. Replacing the urls with \"replacedlink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
    "    txt = str(text)\n",
    "    \n",
    "    # Replace apostrophes with standard lexicons\n",
    "    txt = txt.replace(\"isn't\", \"is not\")\n",
    "    txt = txt.replace(\"aren't\", \"are not\")\n",
    "    txt = txt.replace(\"ain't\", \"am not\")\n",
    "    txt = txt.replace(\"won't\", \"will not\")\n",
    "    txt = txt.replace(\"didn't\", \"did not\")\n",
    "    txt = txt.replace(\"shan't\", \"shall not\")\n",
    "    txt = txt.replace(\"haven't\", \"have not\")\n",
    "    txt = txt.replace(\"hadn't\", \"had not\")\n",
    "    txt = txt.replace(\"hasn't\", \"has not\")\n",
    "    txt = txt.replace(\"don't\", \"do not\")\n",
    "    txt = txt.replace(\"wasn't\", \"was not\")\n",
    "    txt = txt.replace(\"weren't\", \"were not\")\n",
    "    txt = txt.replace(\"doesn't\", \"does not\")\n",
    "    txt = txt.replace(\"'s\", \" is\")\n",
    "    txt = txt.replace(\"'re\", \" are\")\n",
    "    txt = txt.replace(\"'m\", \" am\")\n",
    "    txt = txt.replace(\"'d\", \" would\")\n",
    "    txt = txt.replace(\"'ll\", \" will\")\n",
    "    txt = txt.replace(\"--th\", \" \")\n",
    "    \n",
    "    # More cleaning\n",
    "    txt = re.sub(r\"alot\", \"a lot\", txt)\n",
    "    txt = re.sub(r\"what's\", \"\", txt)\n",
    "    txt = re.sub(r\"What's\", \"\", txt)\n",
    "    \n",
    "    \n",
    "    # Removing URLs\n",
    "    #print('before', txt)\n",
    "    txt= re.sub(r\"\\S*http\\S*\", \"replacedlink\", txt)\n",
    "    txt= re.sub(r\"\\S*.com\\S*\", \"replacedlink\", txt)\n",
    "    \n",
    "    # Removing all symbols\n",
    "    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    #print('after', txt)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    # removing stop words (a, an, the...)   \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stop_words])\n",
    "       \n",
    "    # Stemming (remove -ing, -ly, ...)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "        \n",
    "    # Lemmatisation (convert the word into root word)\n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the clean function for train and test\n",
    "# Lemmatication is performing better than stemming so considering that\n",
    "train['CONTENT'] = train['CONTENT'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = True))\n",
    "test['CONTENT'] = test['CONTENT'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAnalysis for: replacedlink\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "278\n",
      "\n",
      "Total Value Counts: \n",
      "0    931\n",
      "1    191\n",
      "2     24\n",
      "3      8\n",
      "4      2\n",
      "7      1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0      1     2    3    4    7\n",
      "CLASS                                   \n",
      "0      540.0   29.0   2.0  NaN  NaN  NaN\n",
      "1      391.0  162.0  22.0  8.0  2.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mAnalysis for: subscribe\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "161\n",
      "\n",
      "Total Value Counts: \n",
      "0    1014\n",
      "1     126\n",
      "2      16\n",
      "3       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0      1     2    3\n",
      "CLASS                         \n",
      "0      571.0    NaN   NaN  NaN\n",
      "1      443.0  126.0  16.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mAnalysis for: like\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "152\n",
      "\n",
      "Total Value Counts: \n",
      "0    1029\n",
      "1     108\n",
      "2      17\n",
      "3       2\n",
      "4       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0     1     2    3    4\n",
      "CLASS                             \n",
      "0      513.0  54.0   2.0  2.0  NaN\n",
      "1      516.0  54.0  15.0  NaN  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mAnalysis for: watch\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "61\n",
      "\n",
      "Total Value Counts: \n",
      "0    1104\n",
      "1      50\n",
      "2       2\n",
      "7       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0     1    2    7\n",
      "CLASS                       \n",
      "0      548.0  22.0  1.0  NaN\n",
      "1      556.0  28.0  1.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mAnalysis for: please\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "141\n",
      "\n",
      "Total Value Counts: \n",
      "0    1045\n",
      "1      90\n",
      "2      18\n",
      "3       2\n",
      "5       1\n",
      "4       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0     1     2    3    4    5\n",
      "CLASS                                  \n",
      "0      569.0   2.0   NaN  NaN  NaN  NaN\n",
      "1      476.0  88.0  18.0  2.0  1.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[1mAnalysis for: share\u001b[0m\n",
      "\n",
      "Total Number of Occurence: \n",
      "34\n",
      "\n",
      "Total Value Counts: \n",
      "0    1124\n",
      "1      32\n",
      "2       1\n",
      "Name: CONTENT, dtype: int64\n",
      "\n",
      "Comparison Table: \n",
      "\n",
      "freq       0     1    2\n",
      "CLASS                  \n",
      "0      569.0   2.0  NaN\n",
      "1      555.0  30.0  1.0\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the analysis function for different words\n",
    "lis=['replacedlink', 'subscribe', 'like', 'watch','please', 'share']\n",
    "df1=pd.DataFrame()\n",
    "df1['name']=lis\n",
    "df1['name'].apply(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The occurence of 'replacedlink' roughly translates to 85% chance of being an ad which makes sense as the ad usually has a weblink asking the user to visit it. Not removing URLs was a good decision\n",
    "2. If 'subscribe' occurs, it is always an ad.\n",
    "3. Despite having lesser occurence, 'please' and 'share' have more than 90% chance of being ad if they occur.\n",
    "4. My hypothesis is correct and sentences containing these words are more likely to be an ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bag of Words Vector\n",
    "#CountVec = CountVectorizer(max_features = 5000, ngram_range=(1,1))\n",
    "#Countvec_train = CountVec.fit_transform(train['CONTENT']).toarray()\n",
    "#Countvec_test = CountVec.transform(test['CONTENT']).toarray()\n",
    "#print(Countvec_train .shape, Countvec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157, 2254) (799, 2254)\n"
     ]
    }
   ],
   "source": [
    "# Creating Tf-idf vector\n",
    "# For the final, tf-idf is used as it was perfomring better\n",
    "tf = TfidfVectorizer(max_features = 5000, ngram_range=(1,1))\n",
    "tfidf_train = tf.fit_transform(train['CONTENT']).toarray()\n",
    "tfidf_test = tf.transform(test['CONTENT']).toarray()\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-validation split\n",
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First training the data on 66% of actual training data for validation\n",
    "# Since the test size is not so different from training data, for final model, we will be using the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for validation\n",
    "x = tfidf_train\n",
    "y = train['CLASS']\n",
    "trainx, testx, trainy, testy = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8844634628044766\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb_model = MultinomialNB().fit(trainx,trainy)\n",
    "nb_output = nb_model.predict(testx)\n",
    "print(roc_auc_score(testy, nb_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934990125082291\n"
     ]
    }
   ],
   "source": [
    "# XGB \n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic')\n",
    "xgb_model.fit(trainx,trainy)\n",
    "xgb_output = xgb_model.predict(testx)\n",
    "print(roc_auc_score(testy, xgb_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9427803379416283\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier - optimised\n",
    "rf_model = RandomForestClassifier(random_state= 42,n_estimators=500, criterion = \"entropy\")\n",
    "rf_model.fit(trainx,trainy)\n",
    "rf_output = rf_model.predict(testx)\n",
    "print(roc_auc_score(testy, rf_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model created using all the training data, because of comparable train-test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and hyperparameters which we are using here are the one we created during Train-Validation Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier on whole data - optimised \n",
    "rf_model = RandomForestClassifier(random_state= 42,n_estimators=500, criterion = \"entropy\")\n",
    "rf_model.fit(tfidf_train,train['CLASS'])\n",
    "rf_output = rf_model.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the predicted values in csv file\n",
    "# this is final output for the optimised machine learning algorithm\n",
    "final_prediction = pd.DataFrame(test['ID'].copy())\n",
    "final_prediction['CLASS']= rf_output\n",
    "final_prediction.to_csv(\"Adarsh_Singh_15_classification.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tokenizer and word embeddings to make the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings and use of neural networks gives better reuslt as it includes the semantic meaning\n",
    "# we will use lstm to take advantage of its retaining power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (1157, 100)\n",
      "Shape of test data tensor: (799, 100)\n",
      "2272\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and Padding \n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 3000\n",
    "\n",
    "tokenizer = Tokenizer(lower=False,num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['CONTENT'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train['CONTENT'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['CONTENT'])\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "\n",
    "\n",
    "nb_words = (np.max(train_data) + 1)\n",
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 50)           113600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 174,101\n",
      "Trainable params: 174,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding Vector\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,50,input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting embeddings data into train and validation\n",
    "X = train_data\n",
    "Y = train['CLASS']\n",
    "trainx, testx, trainy, testy = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n",
      "E:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 620 samples, validate on 155 samples\n",
      "Epoch 1/10\n",
      "620/620 [==============================] - 12s 19ms/step - loss: 0.5801 - accuracy: 0.7548 - val_loss: 0.3220 - val_accuracy: 0.9161\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 11s 18ms/step - loss: 0.2103 - accuracy: 0.9210 - val_loss: 0.1512 - val_accuracy: 0.9484\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 15s 24ms/step - loss: 0.0982 - accuracy: 0.9677 - val_loss: 0.1424 - val_accuracy: 0.9484\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 18s 30ms/step - loss: 0.0602 - accuracy: 0.9806 - val_loss: 0.1661 - val_accuracy: 0.9419\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 12s 19ms/step - loss: 0.0527 - accuracy: 0.9823 - val_loss: 0.2130 - val_accuracy: 0.9290\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 14s 23ms/step - loss: 0.0363 - accuracy: 0.9871 - val_loss: 0.1830 - val_accuracy: 0.9355\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 10s 16ms/step - loss: 0.0234 - accuracy: 0.9935 - val_loss: 0.1593 - val_accuracy: 0.9613\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 9s 15ms/step - loss: 0.0153 - accuracy: 0.9952 - val_loss: 0.1690 - val_accuracy: 0.9548\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 9s 15ms/step - loss: 0.0141 - accuracy: 0.9968 - val_loss: 0.1729 - val_accuracy: 0.9355\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 9s 15ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 0.2005 - val_accuracy: 0.9355\n",
      "0.9691683124862849\n"
     ]
    }
   ],
   "source": [
    "# neural network on validation dataset\n",
    "model.fit(trainx, trainy, validation_split=0.2, nb_epoch=10, batch_size=5)\n",
    "result = model.predict(testx)\n",
    "print(roc_auc_score(testy,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 925 samples, validate on 232 samples\n",
      "Epoch 1/10\n",
      " 10/925 [..............................] - ETA: 14s - loss: 0.0088 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925/925 [==============================] - 14s 15ms/step - loss: 0.1251 - accuracy: 0.9611 - val_loss: 0.0787 - val_accuracy: 0.9741\n",
      "Epoch 2/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0350 - accuracy: 0.9892 - val_loss: 0.0893 - val_accuracy: 0.9741\n",
      "Epoch 3/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0159 - accuracy: 0.9978 - val_loss: 0.0948 - val_accuracy: 0.9698\n",
      "Epoch 4/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0105 - accuracy: 0.9989 - val_loss: 0.1247 - val_accuracy: 0.9698\n",
      "Epoch 5/10\n",
      "925/925 [==============================] - 15s 16ms/step - loss: 0.0200 - accuracy: 0.9924 - val_loss: 0.0893 - val_accuracy: 0.9784\n",
      "Epoch 6/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.1084 - val_accuracy: 0.9741\n",
      "Epoch 7/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0089 - accuracy: 0.9989 - val_loss: 0.1213 - val_accuracy: 0.9741\n",
      "Epoch 8/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.1292 - val_accuracy: 0.9741\n",
      "Epoch 9/10\n",
      "925/925 [==============================] - 14s 15ms/step - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.1370 - val_accuracy: 0.9741\n",
      "Epoch 10/10\n",
      "925/925 [==============================] - 15s 17ms/step - loss: 0.0068 - accuracy: 0.9989 - val_loss: 0.1414 - val_accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "# running neural network with validation parameters on whole data\n",
    "model.fit(train_data, train['CLASS'], validation_split=0.2, nb_epoch=10, batch_size=5)\n",
    "result_nn = model.predict(test_data)\n",
    "# converting the probabilites scores into 0 or 1\n",
    "result_nn = [1 if i>0.5 else 0 for i in result_nn] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv for neural network\n",
    "# this is the finla output for neural network using lstm\n",
    "final_prediction_nn = pd.DataFrame(test['ID'].copy())\n",
    "final_prediction_nn['CLASS']= result_nn\n",
    "final_prediction_nn.to_csv(\"Adarsh_Singh_16_classification.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
